
# Requisitos
1. Conversão do formato dos arquivos: Converter o arquivo CSV presente no diretório data/input/users/load.csv,
para um formato colunar de alta performance de leitura de sua escolha. Justificar brevemente a escolha do formato;

Comentários Silvio:

Escolhi utilizar o formato de dados Parquet devido a suas vantagens e características.
Este formato foi desenvolvido para ser utilizado em sistemas baseados em Hadoop, com o objetivo de obter alta performance.
A sua estrutura interna é composta por chunks de dados o que permite executar eficientemente tarefas deleitura a escrita sem ter que processar o arquivo por inteiro.
O Parquet também de forma automática adiciona a informação do esquema diferentemente do CSV que não o faz.


2. Deduplicação dos dados convertidos: No conjunto de dados convertidos haverão múltiplas entradas para um mesmo registro,
variando apenas os valores de alguns dos campos entre elas. Será necessário realizar um processo de deduplicação destes dados,
a fim de apenas manter a última entrada de cada registro, usando como referência o id para identificação dos registros duplicados
 e a data de atualização (update_date) para definição do registro mais recente;


3. Conversão do tipo dos dados deduplicados: No diretório config haverá um arquivo
JSON de configuração (types_mapping.json), contendo os nomes dos campos e os respectivos tipos desejados de output.
Utilizando esse arquivo como input, realizar um processo de conversão dos tipos dos campos descritos, no conjunto de dados deduplicados;

# Notas gerais
- Todas as operações devem ser realizadas utilizando Spark. O serviço de execução fica a seu critério,
podendo utilizar tanto serviços locais como serviços em cloud. Justificar brevemente o serviço escolhido (EMR, Glue, Zeppelin, etc.).

Comentários Silvio:

Escolhi utilizar a serviço local devido a uma questão de tempo, já tinha o ambiente configurado na minha máquina.
Aproveitei para concentrar-me na solução.


- Cada operação deve ser realizada no dataframe resultante do passo anterior, podendo ser persistido e
carregado em diferentes conjuntos de arquivos após cada etapa ou executados em memória e apenas persistido após operação final.

- Você tem liberdade p/ seguir a sequência de execução desejada;

- Solicitamos a transformação de tipos de dados apenas de alguns campos. Os outros ficam a seu critério

- O arquivo ou o conjunto de arquivos finais devem ser compactados e enviados por e-mail.